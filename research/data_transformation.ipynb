{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nikhil\\\\OneDrive\\\\Desktop\\\\ML Projects\\\\ipp\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nikhil\\\\OneDrive\\\\Desktop\\\\ML Projects\\\\ipp'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir : Path\n",
    "    train_data_path : Path\n",
    "    test_data_path : Path\n",
    "    preprocessor_obj_file_path : Path\n",
    "    prepared_train_data : Path\n",
    "    prepared_test_data : Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurancePP.constants import *\n",
    "from insurancePP.utils.common import read_yaml, create_directories, save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            preprocessor_obj_file_path = config.preprocessor_obj_file_path,\n",
    "            prepared_train_data = config.prepared_train_data,\n",
    "            prepared_test_data = config.prepared_test_data\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from insurancePP.logging import logger\n",
    "from insurancePP.utils.common import get_size\n",
    "from datasets import load_from_disk\n",
    "from insurancePP.logging import logger\n",
    "\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        try:\n",
    "            categorical_columns = ['sex', 'smoker', 'region']\n",
    "            numerical_columns = ['age', 'bmi', 'children']\n",
    "            logger.info(\"Numerical and Categorical features has been extracted from dataset\")\n",
    "\n",
    "            logger.info(\"creating categorical pipeline\")\n",
    "            categorical_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder()),\n",
    "                ('scaler', StandardScaler(with_mean=False))\n",
    "            ])\n",
    "\n",
    "            logger.info(\"creating numerical pipelines\")\n",
    "            numerical_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "\n",
    "            logger.info(\"creating column transformer object\")\n",
    "            preprocessor = ColumnTransformer([\n",
    "                ('numerical_pipeline', numerical_pipeline, numerical_columns),\n",
    "                ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "            ])\n",
    "\n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        try:\n",
    "            train_df = pd.read_csv(self.config.train_data_path)\n",
    "            test_df = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "            logger.info('obtaining preprocessing object')\n",
    "            preprocessor_obj = self.get_data_transformer_object()\n",
    "\n",
    "            target_column_name = 'expenses'\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "            logger.info('Applying preprocessing object on training and testing dataframe')\n",
    "\n",
    "            input_feature_train_arr = preprocessor_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessor_obj.transform(input_feature_test_df)\n",
    "\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            save(self.config.prepared_train_data, train_arr)\n",
    "            save(self.config.prepared_test_data, test_arr)\n",
    "\n",
    "            save_object(\n",
    "                file_path = Path(self.config.preprocessor_obj_file_path),\n",
    "                obj = preprocessor_obj\n",
    "            )\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-25 18:53:02,160 : INFO : common : yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-25 18:53:02,164 : INFO : common : yaml file: params.yaml loaded successfully]\n",
      "[2024-02-25 18:53:02,168 : INFO : common : directory artifacts created]\n",
      "[2024-02-25 18:53:02,170 : INFO : common : directory artifacts/data_transformation created]\n",
      "[2024-02-25 18:53:02,172 : INFO : 1099139855 : Numerical and Categorical features has been extracted from dataset]\n",
      "[2024-02-25 18:53:02,174 : INFO : 1099139855 : creating categorical pipeline]\n",
      "[2024-02-25 18:53:02,175 : INFO : 1099139855 : creating numerical pipelines]\n",
      "[2024-02-25 18:53:02,177 : INFO : 1099139855 : creating column transformer object]\n",
      "[2024-02-25 18:53:02,189 : INFO : 1099139855 : obtaining preprocessing object]\n",
      "[2024-02-25 18:53:02,190 : INFO : 1099139855 : Numerical and Categorical features has been extracted from dataset]\n",
      "[2024-02-25 18:53:02,193 : INFO : 1099139855 : creating categorical pipeline]\n",
      "[2024-02-25 18:53:02,195 : INFO : 1099139855 : creating numerical pipelines]\n",
      "[2024-02-25 18:53:02,197 : INFO : 1099139855 : creating column transformer object]\n",
      "[2024-02-25 18:53:02,203 : INFO : 1099139855 : Applying preprocessing object on training and testing dataframe]\n",
      "[2024-02-25 18:53:02,270 : INFO : common : Binary Object is stored]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config = data_transformation_config)\n",
    "    data_transformation.get_data_transformer_object()\n",
    "    data_transformation.initiate_data_transformation()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
